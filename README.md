Agentic RAG Project (Open Source)

This project implements an Agentic RAG pipeline using LangGraph
, Weaviate
, and open-source HuggingFace models only (no billing, no external APIs).
Each agent is modular, making the system easy to extend.

âš™ï¸ Features

Orchestrator Agent â†’ Guardrails (PII redaction, blocklist).

Intent Router â†’ Simple query classification.

Planner / Decomposer â†’ Breaks down multi-step queries.

Retrieval Agent â†’ Weaviate dense vector search + Cross-Encoder reranking.

Synthesizer Agent â†’ Generates grounded answers using open-source LLM (Flan-T5).

Validator Agent â†’ Checks grounding (citations present).

All components are modular and live in agents/, models/, utils/.

ğŸ“‚ Project Structure
agentic_rag_project/
â”‚
â”œâ”€â”€ configs
â”œâ”€â”€ docs
â”œâ”€â”€ evals
â”œâ”€â”€ notebooks
â”œâ”€â”€ indexing
â”œâ”€â”€ parsing
â”œâ”€â”€ scraping
â”œâ”€â”€ src/
    â”œâ”€â”€main.py

|    â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â”œâ”€â”€ intent_router.py
â”‚   â”œâ”€â”€ planner.py
â”‚   â”œâ”€â”€ retrieval.py
â”‚   â”œâ”€â”€ synthesizer.py
â”‚   â”œâ”€â”€ validator.py
â”‚
    â”œâ”€â”€  models/
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ reranker.py
â”‚   â”œâ”€â”€ generator.py
â”‚
    â”œâ”€â”€  utils/
    â”œâ”€â”€ chunking.py
    â”œâ”€â”€ config.py
    â”œâ”€â”€ helpers.py
â”‚   â”œâ”€â”€ weaviate_client.py
â”‚   â”œâ”€â”€ guardrails.py
â”‚   â””â”€â”€ logger.py
â”‚   
    â”œâ”€â”€api/
    route.py

â””â”€â”€ requirements.txt

ğŸ› ï¸ Installation
1. Clone repo
git clone <repo>
cd agentic_rag_project

2. Create environment
python -m venv .venv
source .venv/bin/activate   # on Windows: .venv\Scripts\activate

3. Install dependencies
pip install -r requirements.txt

4. Run Weaviate locally

If you donâ€™t already have Weaviate running:

docker-compose.yml

OR

docker run -d --name weaviate \
  -p 8080:8080 \
  semitechnologies/weaviate:1.25.3 \
  --host 0.0.0.0 --port 8080 \
  --persistency.data-path /var/lib/weaviate

ğŸ“¥ Indexing Documents

Prepare your chunks (JSONL).
Run indexing:

python src/indexing/weaviate_index.py


This will embed chunks with all-MiniLM-L6-v2 and insert them into the DocChunk class.

ğŸš€ Running the Agents

Once chunks are indexed:


ğŸŒ Run as API (FastAPI + Uvicorn)
1. Make sure src/ has an __init__.py so it is importable:
src/
 â”œâ”€â”€ __init__.py   # empty file
 â”œâ”€â”€ api/
    â”œâ”€â”€ route.py
 â”œâ”€â”€ main.py
 ...

2. Start the API server

From project root:

uvicorn src.api:app --reload --host 0.0.0.0 --port 8000

3. Test in Browser

Open http://localhost:8000/docs
 â†’ Swagger UI.

Or use curl:

curl -X POST "http://localhost:8000/query" \
     -H "Content-Type: application/json" \
     -d '{"query": "Compare treatment A with treatment B"}'


Response:

{
  "final_answer": "Answer [chunk_23]. Answer[chunk_41].",
  "error": null



ğŸ”§ Customization

Change Generator Model â†’ in models/generator.py (default = flan-t5-base, replace with zephyr, falcon, etc).

Change Reranker Model â†’ in models/reranker.py (cross-encoder/ms-marco-MiniLM-L-6-v2 is fast, you can upgrade to electra-base for more accuracy).

Change Embedding Model â†’ in models/embeddings.py.

Schema â†’ matches weaviate_index.py (DocChunk class).

âœ… Roadmap

Add RAGAS Evaluator for automatic faithfulness scoring.

Extend Intent Router with HuggingFace zero-shot classifier.

Add conversation memory for multi-turn queries.

ğŸ“ License

MIT License â€” free to use, modify, extend.